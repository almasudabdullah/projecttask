{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcyaZwyp3C30lh54ptGzRy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almasudabdullah/projecttask/blob/main/Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text extraction from Pdf**"
      ],
      "metadata": {
        "id": "xQcLCKFh702C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTdcm2YYadIc"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "\n",
        "# Text extraction from Pdf\n",
        "\n",
        "import PyPDF2\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            num_pages = len(pdf_reader.pages)\n",
        "            for page_number in range(num_pages):\n",
        "                page = pdf_reader.pages[page_number]\n",
        "                text += page.extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "    return text\n",
        "# PDF\n",
        "pdf_path = \"/content/Story of Prophets.pdf\"\n",
        "\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "print(\"Extracted Text:\\n\", extracted_text)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking by sentences**"
      ],
      "metadata": {
        "id": "vssq0pZi8AoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "def chunk_text_by_sentences(extracted_text):\n",
        "    sentences = sent_tokenize(extracted_text)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# Chunking by sentences\n",
        "text_chunks = chunk_text_by_sentences(extracted_text)\n",
        "\n",
        "# Print each sentence (chunk)\n",
        "for i, sentence in enumerate(text_chunks, 1):\n",
        "    print(f\"Sentence {i}:\\n{sentence}\\n{'='*20}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Tbad3ikQdxbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "t_fo3zzjiiSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding with Sentence Transformers**"
      ],
      "metadata": {
        "id": "IiwpCjnw8H_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "def embed_text_chunks(text_chunks, model_name=\"paraphrase-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embedded_chunks = model.encode(text_chunks)\n",
        "    return embedded_chunks\n",
        "# Third portion: Embedding with Sentence Transformers\n",
        "embedded_chunks = embed_text_chunks(text_chunks)\n",
        "\n",
        "# Print embedded chunks\n",
        "for i, embedded_chunk in enumerate(embedded_chunks, 1):\n",
        "    print(f\"Embedded Chunk {i}:\\n{embedded_chunk}\\n{'='*20}\\n\")"
      ],
      "metadata": {
        "id": "xURzFzqZipnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank chunks by cosine similarity to the query**"
      ],
      "metadata": {
        "id": "J4QMt9uR8Ocy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def rank_chunks_by_similarity(query_vector, embedded_chunks, top_n=5):\n",
        "    similarity_scores = cosine_similarity(query_vector.reshape(1, -1), embedded_chunks).flatten()\n",
        "    ranked_indices = np.argsort(similarity_scores)[::-1][:top_n]\n",
        "    return ranked_indices, similarity_scores[ranked_indices]\n",
        "\n",
        "# Enter a query text\n",
        "query_text = \"Jesus\"\n",
        "\n",
        "# Embed the query text\n",
        "query_vector = embed_text_chunks([query_text])[0]\n",
        "\n",
        "# Rank chunks by cosine similarity to the query\n",
        "top_n = 10\n",
        "ranked_indices, similarity_scores = rank_chunks_by_similarity(query_vector, embedded_chunks, top_n)\n",
        "\n",
        "# Print the top-ranked chunks and their similarity scores\n",
        "print(\"\\nTop\", top_n, \"Ranked Chunks:\")\n",
        "for i, idx in enumerate(ranked_indices, 1):\n",
        "    print(f\"Rank {i}: Chunk {idx + 1}, Similarity Score: {similarity_scores[i - 1]}\")\n",
        "    print(text_chunks[idx])\n",
        "    print(\"=\"*20)"
      ],
      "metadata": {
        "id": "2DLFd0xclyfH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}